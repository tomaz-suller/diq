{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Define Replacement Dictionary\n",
    "replacement_dict = {\n",
    "    \"streeet space\": \"street space\",\n",
    "    \"street spacemta meters\": \"street space\",\n",
    "    \"street sace\": \"street space\",\n",
    "    \"street spacemta\": \"street space\",\n",
    "}\n",
    "\n",
    "# Step 2: Regex Normalization\n",
    "def apply_regex(text):\n",
    "    if re.search(r\"\\bstreet\\s*space\\b\", text):\n",
    "        return \"street space\"\n",
    "    elif re.search(r\"\\binspection\\b\", text):\n",
    "        return \"inspections\"\n",
    "    elif re.search(r\"\\breroofing\\b\", text):\n",
    "        return \"reroofing\"\n",
    "    elif re.search(r\"\\breroof\\b\", text):\n",
    "        return \"reroofing\"\n",
    "    elif re.search(r\"\\bsoft story retrofit\\b\", text):\n",
    "        return \"soft story retrofit\"\n",
    "    elif re.search(r\"provide.*?(sprinkler|sprinklers).*?monitoring system.*?water flow.*?(valve monitoring|tamper monitoring)\", text):\n",
    "        return \"provide sprinkler system monitoring\"\n",
    "    return text\n",
    "\n",
    "# Step 3: Text Cleaning and Normalization\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_text(text, replacement_dict):\n",
    "    for old_term, new_term in replacement_dict.items():\n",
    "        text = text.replace(old_term, new_term)\n",
    "    return apply_regex(text)\n",
    "\n",
    "# Step 4: Dimensionality Reduction with t-SNE\n",
    "def compute_tsne_groups(texts, threshold=0.8, perplexity=30, random_state=42):\n",
    "    print(\"Reducing Dimensionality with t-SNE...\")\n",
    "    \n",
    "    # Create embeddings using character-level vectors\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3), stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Convert sparse matrix to dense matrix\n",
    "    dense_matrix = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    standardized_matrix = scaler.fit_transform(dense_matrix)\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, init=\"random\")\n",
    "    tsne_embeddings = tsne.fit_transform(standardized_matrix)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity(tsne_embeddings)\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    print(\"Creating Groups Using Cosine Similarity...\")\n",
    "    for i in tqdm(range(len(sim_matrix)), desc=\"Assigning Groups\"):\n",
    "        similar_indices = np.where(sim_matrix[i] > threshold)[0]\n",
    "        for j in similar_indices:\n",
    "            if i != j:  # Avoid self-matching\n",
    "                groups[i].append(j)\n",
    "    return groups\n",
    "\n",
    "# Step 5: Assign Groups to DataFrame\n",
    "def assign_groups(df, groups):\n",
    "    group_labels = {}\n",
    "    group_id = 1\n",
    "\n",
    "    print(\"Assigning Groups to DataFrame...\")\n",
    "    for leader, members in tqdm(groups.items(), desc=\"Processing Groups\"):\n",
    "        for member in members:\n",
    "            group_labels[member] = f\"Group_{group_id}\"\n",
    "        group_labels[leader] = f\"Group_{group_id}\"\n",
    "        group_id += 1\n",
    "\n",
    "    df[\"group_label\"] = df.index.map(group_labels).fillna(\"No Group\")\n",
    "    return df\n",
    "\n",
    "# Step 6: Main Pipeline\n",
    "def main_pipeline(file_path, column, threshold=0.8, batch_size=5000, shuffle=True):\n",
    "    print(\"Loading Dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Shuffle Dataset\n",
    "    if shuffle:\n",
    "        print(\"Shuffling Dataset...\")\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"Cleaning and Normalizing Text...\")\n",
    "    df[\"cleaned_description\"] = df[column].apply(clean_text)\n",
    "    df[\"normalized_description\"] = df[\"cleaned_description\"].apply(lambda x: normalize_text(x, replacement_dict))\n",
    "\n",
    "    print(\"Deduplicating Data...\")\n",
    "    unique_texts = df[\"normalized_description\"].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Process in batches to save memory\n",
    "    num_batches = (len(unique_texts) // batch_size) + 1\n",
    "    combined_groups = defaultdict(list)\n",
    "\n",
    "    print(f\"Processing in {num_batches} batches...\")\n",
    "    for batch_id in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        start_idx = batch_id * batch_size\n",
    "        end_idx = min((batch_id + 1) * batch_size, len(unique_texts))\n",
    "        batch_texts = unique_texts[start_idx:end_idx]\n",
    "\n",
    "        batch_groups = compute_tsne_groups(batch_texts.tolist(), threshold=threshold)\n",
    "        for key, values in batch_groups.items():\n",
    "            combined_groups[start_idx + key] = [start_idx + v for v in values]\n",
    "\n",
    "    print(\"Assigning Groups to Dataset...\")\n",
    "    grouped_df = assign_groups(df, combined_groups)\n",
    "\n",
    "    print(\"Saving Results...\")\n",
    "    output_file = f\"grouped_descriptions_tsne_{threshold}.csv\"\n",
    "    grouped_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    return grouped_df\n",
    "\n",
    "# Run the Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/satvikbisht/Documents/Polimi/Semester 3/Data Quality /Project/diq/data/raw/building_permits.csv\"\n",
    "    threshold = 0.8\n",
    "    batch_size = 10000\n",
    "    df = main_pipeline(file_path, column=\"Description\", threshold=threshold, batch_size=batch_size)\n",
    "    print(\"Processing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique group labels\n",
    "unique_groups = df[\"group_label\"].unique()\n",
    "print(\"Unique Group Labels:\")\n",
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows belonging to Group_1\n",
    "group_1_df = df[df[\"group_label\"] == \"Group_121257\"]\n",
    "\n",
    "# Display descriptions in Group_1\n",
    "print(\"Descriptions in Group_1:\")\n",
    "print(group_1_df[\"Description\"].tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXISTING USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t8/c3gz51w107s3mw7rbv_100lr0000gn/T/ipykernel_23178/2415644177.py:99: DtypeWarning: Columns (22,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling Dataset...\n",
      "Cleaning and Normalizing Text...\n",
      "Deduplicating Data...\n",
      "Processing in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing Dimensionality with t-SNE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satvikbisht/Documents/AI/MLOPS/MLOPS-E2E-Project/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:543: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Groups Using Cosine Similarity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Groups: 100%|██████████| 93/93 [00:00<00:00, 264275.25it/s]\n",
      "Processing Batches: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning Groups to Dataset...\n",
      "Assigning Groups to DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Groups: 100%|██████████| 93/93 [00:00<00:00, 397625.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to grouped_Existing_Use_tsne_0.8.csv\n",
      "Processing Complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Define Replacement Dictionary\n",
    "replacement_dict = {\n",
    "    \"streeet space\": \"street space\",\n",
    "    \"street spacemta meters\": \"street space\",\n",
    "    \"street sace\": \"street space\",\n",
    "    \"street spacemta\": \"street space\",\n",
    "}\n",
    "\n",
    "# Step 2: Regex Normalization\n",
    "def apply_regex(text):\n",
    "    if re.search(r\"\\bstreet\\s*space\\b\", text):\n",
    "        return \"street space\"\n",
    "    elif re.search(r\"\\binspection\\b\", text):\n",
    "        return \"inspections\"\n",
    "    elif re.search(r\"\\breroofing\\b\", text):\n",
    "        return \"reroofing\"\n",
    "    elif re.search(r\"\\breroof\\b\", text):\n",
    "        return \"reroofing\"\n",
    "    elif re.search(r\"\\bsoft story retrofit\\b\", text):\n",
    "        return \"soft story retrofit\"\n",
    "    elif re.search(r\"provide.*?(sprinkler|sprinklers).*?monitoring system.*?water flow.*?(valve monitoring|tamper monitoring)\", text):\n",
    "        return \"provide sprinkler system monitoring\"\n",
    "    return text\n",
    "\n",
    "# Step 3: Text Cleaning and Normalization\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_text(text, replacement_dict):\n",
    "    for old_term, new_term in replacement_dict.items():\n",
    "        text = text.replace(old_term, new_term)\n",
    "    return apply_regex(text)\n",
    "\n",
    "# Step 4: Dimensionality Reduction with t-SNE\n",
    "def compute_tsne_groups(texts, threshold=0.8, perplexity=30, random_state=42):\n",
    "    print(\"Reducing Dimensionality with t-SNE...\")\n",
    "    \n",
    "    # Create embeddings using character-level vectors\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3), stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Convert sparse matrix to dense matrix\n",
    "    dense_matrix = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    standardized_matrix = scaler.fit_transform(dense_matrix)\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state, init=\"random\")\n",
    "    tsne_embeddings = tsne.fit_transform(standardized_matrix)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity(tsne_embeddings)\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    print(\"Creating Groups Using Cosine Similarity...\")\n",
    "    for i in tqdm(range(len(sim_matrix)), desc=\"Assigning Groups\"):\n",
    "        similar_indices = np.where(sim_matrix[i] > threshold)[0]\n",
    "        for j in similar_indices:\n",
    "            if i != j:  # Avoid self-matching\n",
    "                groups[i].append(j)\n",
    "    return groups\n",
    "\n",
    "# Step 5: Assign Groups to DataFrame\n",
    "def assign_groups(df, groups):\n",
    "    group_labels = {}\n",
    "    group_id = 1\n",
    "\n",
    "    print(\"Assigning Groups to DataFrame...\")\n",
    "    for leader, members in tqdm(groups.items(), desc=\"Processing Groups\"):\n",
    "        for member in members:\n",
    "            group_labels[member] = f\"Group_{group_id}\"\n",
    "        group_labels[leader] = f\"Group_{group_id}\"\n",
    "        group_id += 1\n",
    "\n",
    "    df[\"group_label\"] = df.index.map(group_labels).fillna(\"No Group\")\n",
    "    return df\n",
    "\n",
    "# Step 6: Main Pipeline\n",
    "def main_pipeline(file_path, column, threshold=0.8, batch_size=5000, shuffle=True):\n",
    "    print(\"Loading Dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Shuffle Dataset\n",
    "    if shuffle:\n",
    "        print(\"Shuffling Dataset...\")\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"Cleaning and Normalizing Text...\")\n",
    "    df[\"cleaned_Existing_Use\"] = df[column].apply(clean_text)\n",
    "    df[\"normalized__Existing_Use\"] = df[\"cleaned_Existing_Use\"].apply(lambda x: normalize_text(x, replacement_dict))\n",
    "\n",
    "    print(\"Deduplicating Data...\")\n",
    "    unique_texts = df[\"normalized__Existing_Use\"].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Process in batches to save memory\n",
    "    num_batches = (len(unique_texts) // batch_size) + 1\n",
    "    combined_groups = defaultdict(list)\n",
    "\n",
    "    print(f\"Processing in {num_batches} batches...\")\n",
    "    for batch_id in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        start_idx = batch_id * batch_size\n",
    "        end_idx = min((batch_id + 1) * batch_size, len(unique_texts))\n",
    "        batch_texts = unique_texts[start_idx:end_idx]\n",
    "\n",
    "        batch_groups = compute_tsne_groups(batch_texts.tolist(), threshold=threshold)\n",
    "        for key, values in batch_groups.items():\n",
    "            combined_groups[start_idx + key] = [start_idx + v for v in values]\n",
    "\n",
    "    print(\"Assigning Groups to Dataset...\")\n",
    "    grouped_df = assign_groups(df, combined_groups)\n",
    "\n",
    "    print(\"Saving Results...\")\n",
    "    output_file = f\"grouped_Existing_Use_tsne_{threshold}.csv\"\n",
    "    grouped_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    return grouped_df\n",
    "\n",
    "# Run the Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/satvikbisht/Documents/Polimi/Semester 3/Data Quality /Project/diq/data/raw/building_permits.csv\"\n",
    "    threshold = 0.8\n",
    "    batch_size = 10000\n",
    "    df = main_pipeline(file_path, column=\"Existing Use\", threshold=threshold, batch_size=batch_size)\n",
    "    print(\"Processing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Group Labels:\n",
      "['Group_87' 'Group_92' 'Group_79' 'Group_93' 'Group_89' 'Group_82'\n",
      " 'Group_91' 'No Group']\n"
     ]
    }
   ],
   "source": [
    "# Display unique group labels\n",
    "unique_groups = df[\"group_label\"].unique()\n",
    "print(\"Unique Group Labels:\")\n",
    "print(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions in Group_1:\n",
      "['1)interior t.i. 1st & 2nd floor. demo old restrooms at both flrs & add new restrooms, shower, & kchn at 1st flr. 2)new store front system with new entry door on 7th street. 3)nw swing dr on gilbert st. 4)voluntary seismic upgrade. elect. mech. under seperate permit.n/a for the maher.', 'kitchen cabinet, install 4 cabinets and counter top, hook up stove, same location, install 2 counter plugs and hood. no structural work.', 'street space permit  - 201705126445', 'street space permit  (renewal of permit # 1319329)', 'new natural gas powered emergency generator  at lower level.', 'install a fire alarm system. ref pa 2015-0410-3363', 'like for like replacement of one chiller. bms upgrade.', 'replace 11 windows on back and sides of house.  no structural changes.  max u-factor .40.  2 windows on right side of house to be anderson paintable fibrex, all others vinyl. all at existing openings at upper level.', 'revision to bpa no. 201404183624  replacing 34 windows. 15 marvin (wood base, aluminum clad) windows at front face of building. replace 19 simonton vinyl windows at rear locations not visible from street. all windows to be double-hung same size. no altering to existing opening.', 'remove & replace deck railing & boards in kind. front and rear, 8 locations, less than 50%.', 'adu conversion at garage level. new sprinkler and rear facade work. ordinance number 162-16**maher n/a**', 'kitchen remodel: no sturctural wok, replace in kind. replace cabinets, coutertops, appliances, plumbing locations to remain, exisitng applainace elecrical outlets same location. bathroom remodel: replace vanity w/ new in same location.', '1) relocate extg hand sink. 2)upgrade extg restroom t ada restroom.', 'reroofing', 'street space', 'kitchen remodel like for like replace kitchen cabinets counter top fill back splash. replace front four windows and two side windows. wood windows within the existing openings. single or double hung operation.', 'ground fl: installing 4g cell comunicator to existing fire alarm system for primary communications to central station.', 'erect (n) 4-story commercial/office building.building is on (2) lots. need address change before issuance.']\n"
     ]
    }
   ],
   "source": [
    "# Filter rows belonging to Group_1\n",
    "group_1_df = df[df[\"group_label\"] == \"Group_93\"]\n",
    "\n",
    "# Display descriptions in Group_1\n",
    "print(\"Descriptions in Group_1:\")\n",
    "print(group_1_df[\"Description\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37a30ca0ea4b6e7c3e62151ccbd94dbb6a7efabcf790d4531e66a4bcc466a853"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
